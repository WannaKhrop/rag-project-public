{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc66718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# do not forget to start ChromaDB using Docker\n",
    "os.environ[\"CHROMA_DB_URL\"] = \"localhost\"\n",
    "os.environ[\"CHROMA_DB_PORT\"] = \"8000\"\n",
    "\n",
    "from src.evaluation import read_df, METRIC_COLUMNS, MIN_RERANKING_SCORE\n",
    "import pandas as pd\n",
    "from src.vector_db import check_document_existance\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0dc43b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data\n",
    "DATA_PATH = \"data/Run 7.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efea05c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read DataFrames\n",
    "questions = read_df()\n",
    "evaluation_results = pd.read_csv(filepath_or_buffer=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f86b694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total average result:\n",
      "Context Precision      0.567457\n",
      "Context Recall         0.438350\n",
      "Precision              0.546911\n",
      "Recall                 0.205000\n",
      "Faithfulness           0.591351\n",
      "Response Relevancy     0.479990\n",
      "Factual Correctness    0.181016\n",
      "Semantic Similarity    0.455566\n",
      "Nothing Retrieved      0.336207\n",
      "Best Ranking Value     0.579881\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# current average results\n",
    "print(\"Total average result:\")\n",
    "print(evaluation_results.drop(columns=\"Question\").mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a28ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Columns: ['Question', 'Context Precision', 'Context Recall', 'Precision', 'Recall', 'Faithfulness', 'Response Relevancy', 'Factual Correctness', 'Semantic Similarity', 'Nothing Retrieved', 'Best Ranking Value', 'question', 'correct_answer', 'context']\n",
      "Not answered: 0.336\n"
     ]
    }
   ],
   "source": [
    "# merge DataFrames\n",
    "df = pd.merge(left=evaluation_results, right=questions, right_on=\"question_id\", left_on=\"Question\").drop(columns=\"question_id\")\n",
    "print(\"All Columns:\", list(df.columns))\n",
    "print(\"Not answered:\", round(df[\"Nothing Retrieved\"].mean(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4e8af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to Answered and non-Answered questions\n",
    "df_answered_mask = evaluation_results[\"Nothing Retrieved\"]\n",
    "df_answered = evaluation_results.loc[~df_answered_mask][METRIC_COLUMNS].drop(columns=\"Nothing Retrieved\")\n",
    "df_not_answered = evaluation_results.loc[df_answered_mask][METRIC_COLUMNS].drop(columns=\"Nothing Retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "148df6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results for Answered Questions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Context Precision      0.854870\n",
       "Context Recall         0.663293\n",
       "Precision              0.823918\n",
       "Recall                 0.308831\n",
       "Faithfulness           0.898853\n",
       "Response Relevancy     0.727932\n",
       "Factual Correctness    0.463400\n",
       "Semantic Similarity    0.690891\n",
       "Best Ranking Value     0.797763\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get mean values in case of answered questions\n",
    "print(\"Average Results for Answered Questions:\")\n",
    "df_answered.drop(columns=\"Question\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73f4377a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results for Non-Answered Questions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Context Precision      0.000000\n",
       "Context Recall         0.000000\n",
       "Precision              0.000000\n",
       "Recall                 0.000000\n",
       "Faithfulness           0.000000\n",
       "Response Relevancy     0.000000\n",
       "Factual Correctness    0.000000\n",
       "Semantic Similarity    0.000000\n",
       "Best Ranking Value     0.149704\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get mean values in case of non-answered questions\n",
    "print(\"Average Results for Non-Answered Questions:\")\n",
    "df_not_answered.drop(columns=\"Question\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6eb5e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total 78 questions were not answered.\n"
     ]
    }
   ],
   "source": [
    "# select questions which were not answered\n",
    "not_answered_questions = df_not_answered.Question.unique()\n",
    "print(f\"In total {len(not_answered_questions)} questions were not answered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8030eb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not presented in the database: 0\n",
      "Not retrieved from the database: 78\n"
     ]
    }
   ],
   "source": [
    "# check for each question that chunks actually exist in VectorDB\n",
    "not_presented_in_db, not_retrieved = list(), list()\n",
    "\n",
    "for q_id in not_answered_questions:\n",
    "    if not check_document_existance(doc_name=q_id):\n",
    "        not_presented_in_db.append(q_id)\n",
    "    else:\n",
    "        not_retrieved.append(q_id)\n",
    "\n",
    "print(\"Not presented in the database:\", len(not_presented_in_db))\n",
    "print(\"Not retrieved from the database:\", len(not_retrieved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed14e205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors in response generation: []\n"
     ]
    }
   ],
   "source": [
    "# check DocmentRanking performance for documents which exist in DB but were not retrieved\n",
    "evaluation_results.index = evaluation_results[\"Question\"]\n",
    "bad_ranks = evaluation_results.loc[not_retrieved, \"Best Ranking Value\"]\n",
    "\n",
    "llm_response_generation_errors = bad_ranks.where(lambda x: x > MIN_RERANKING_SCORE).dropna()\n",
    "print(\"Errors in response generation:\", list(llm_response_generation_errors.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff2bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
